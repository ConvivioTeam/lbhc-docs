{ 
  "0": {
    "id": "0",
    "title": "API Gateway",
    "content": " microserviceThe  microservice provides the interface from the public internet into the Directory of Services system. Its main function is handle RESTful requests from outside the system, pass the request (and associated data, as appropriate) into the event stream, and then consume appropriate response(s) in the event stream and handle sending that back in the HTTP reponse.Endpoints are designed to be simple and easy to understand following a standard design pattern:  Endpoints should be plural e.g. providers  Each endpoint must support GET POST and PUT requests  When returning data from the response it should contain the full data object including any updatesRepositoryCode for the DoS  microservice is in the https://github.com/LBHackney-IT/DoS-api-gateway repository.Technology PlatformThe Scraper microservice is built with Laravel Lumen. It is a small PHP application.API Spec docView the spec at SwaggerHubView the raw spec document at https://github.com/LBHackney-IT/DoS-api-gateway/blob/master/docs/api/api-gateway.openapi.yml",
    "url": "/microservices/apigateway",
    "relUrl": "/microservices/apigateway"
  },
  "1": {
    "id": "1",
    "title": "Contacts",
    "content": "Structure            Column      Type      Comment                  id      char(36)                     url      varchar(255) NULL                     email      varchar(255)                     name      varchar(255)                     position      varchar(255) NULL                     social_facebook      varchar(255) NULL                     social_twitter      varchar(255) NULL                     phonenumber      varchar(255) NULL                     created      datetime                     updated      datetime                     flagged      tinyint(1)             ",
    "url": "/microservices/datastore/contacts",
    "relUrl": "/microservices/datastore/contacts"
  },
  "2": {
    "id": "2",
    "title": "Cost Options",
    "content": "Structure            Column      Type      Comment                  id      char(36)                     option      varchar(255)             ",
    "url": "/microservices/datastore/costoptions",
    "relUrl": "/microservices/datastore/costoptions"
  },
  "3": {
    "id": "3",
    "title": "Eligitbilities",
    "content": "Structure            Column      Type      Comment                  id      char(36)                     eligibility      varchar(255)             ",
    "url": "/microservices/datastore/eligibilities",
    "relUrl": "/microservices/datastore/eligibilities"
  },
  "4": {
    "id": "4",
    "title": "Events",
    "content": "Structure            Column      Type      Comment                  id      char(36)                     details      varchar(255)                     time      varchar(255)                     venue_id      char(36)                     created      datetime                     updated      datetime             ",
    "url": "/microservices/datastore/events",
    "relUrl": "/microservices/datastore/events"
  },
  "5": {
    "id": "5",
    "title": "Data Store",
    "content": "Data store: event sourcingThe  microservice is built with Laravel Lumen. It is a small PHP application.The data store microservice uses Laravel queues in Lumen to source jobs from the event store microservice.",
    "url": "/microservices/datastore/eventsourcing",
    "relUrl": "/microservices/datastore/eventsourcing"
  },
  "6": {
    "id": "6",
    "title": "Data Store",
    "content": " MicroserviceStores and retrieves the data of the directory of service.Contains data for:  Providers  Services  Eligibility  Cost options  Events  Contacts  Venues  Taxonomy  Taxonomy ReferenceRepositoryCode for the DoS  microservice is in the https://github.com/LBHackney-IT/DoS-data-store-service repository.Technology PlatformThe  microservice is built with Laravel Lumen. It is a small PHP application.FunctionalityThis service provide the following functionality:  Creation of production and development databases  Migration handler for database updates  Model and Object definitionsBuild and DeploymentDrone CI is used to build and push the Docker image to Amazon ECR. It can be access here.The database build and migration service has the following workflow:  Database changes are made to the app and pushed to master  Drone CI builds the docker image  Drone CI pushes the docker image to AWS ECRFuture work:  Expand CI pipeline to deploy to ECS and run database migrations  Validation of any changes to the database  Reporting of any changes to the database",
    "url": "/microservices/datastore",
    "relUrl": "/microservices/datastore"
  },
  "7": {
    "id": "7",
    "title": "Providers",
    "content": "Structure            Column      Type      Comment                  id      char(36)                     name      varchar(255)                     published      tinyint(1)                     venue_id      char(36) NULL                     contact_id      char(36) NULL                     created      datetime                     updated      datetime                     flagged      tinyint(1)             ",
    "url": "/microservices/datastore/providers",
    "relUrl": "/microservices/datastore/providers"
  },
  "8": {
    "id": "8",
    "title": "Services",
    "content": "Structure            Column      Type      Comment                  id      char(36)                     name      varchar(255)                     desc      varchar(255)                     provider_id      char(36)                     event_id      char(36) NULL                     elegibility_id      char(36) NULL                     costoption_id      char(36) NULL                     created      datetime                     updated      datetime                     flagged      tinyint(1)             ",
    "url": "/microservices/datastore/services",
    "relUrl": "/microservices/datastore/services"
  },
  "9": {
    "id": "9",
    "title": "Taxonomy",
    "content": "Structure            Column      Type      Comment                  id      char(36)                     name      varchar(255)             ",
    "url": "/microservices/datastore/taxonomy",
    "relUrl": "/microservices/datastore/taxonomy"
  },
  "10": {
    "id": "10",
    "title": "Taxonomy Reference",
    "content": "Structure            Column      Type      Comment                  service_id      char(36)                     taxonomy_id      char(36)             ",
    "url": "/microservices/datastore/taxonomy_ref",
    "relUrl": "/microservices/datastore/taxonomy_ref"
  },
  "11": {
    "id": "11",
    "title": "Venues",
    "content": "Structure            Column      Type      Comment                  id      char(36)                     service_id      char(36)                     provider_id      char(36) NULL                     name      varchar(255)                     address      varchar(255)                     details      varchar(255)                     created      datetime                     updated      datetime                     flagged      tinyint(1)             ",
    "url": "/microservices/datastore/venues",
    "relUrl": "/microservices/datastore/venues"
  },
  "12": {
    "id": "12",
    "title": "Event Stream Architecture",
    "content": "The Event Stream microservice is a simple system, consisting of two tools:  Apache Kafka, the event stream itself;  Apache Zookeeper, which maintains all the metadata about the Kafka cluster.There is also a Traefik instance as reverse proxy and load balancer (although, not sure it’s actually needed since  connections to Kafka are over tcp).",
    "url": "/microservices/eventstream/architecture",
    "relUrl": "/microservices/eventstream/architecture"
  },
  "13": {
    "id": "13",
    "title": "Event Sourcing",
    "content": ": Consuming and Publishing EventsEach microservice will need to source events from the event stream.If an app is built with Laravel (most of the microservices are built with Lumen, a Laravel microservices framework), then it will need to use a library to work with the Kafka-powered event stream. and Laravel QueuesLaravel has an effective system for managing queues so Lumen, based on Laravel, can use the queue API.Running the Queue Worker(excerpt from Laravel queue docs on running the queue worker)Laravel includes a queue worker that will process new jobs as they are pushed onto the queue. You may run the worker using the queue:work Artisan command. Note that once the  queue:work command has started, it will continue to run until it is manually stopped or you close your terminal:$ php artisan queue:workTo keep the queue:work process running permanently in the background, you should use a process monitor such as Supervisor to ensure that the queue worker does not stop running.PHP + Kafka LibrariesThis project looks like it could be a useful driver to add to a Laravel Lumen microservice:  Kafka Queue driver for Laravel: https://github.com/rapideinternet/laravel-queue-kafkaSupported versions of LaravelTested on: [5.4, 5.5, 5.6, 5.7]AlternativesSee also:  https://github.com/arquivei/laravel-kafka-queue-connector  https://github.com/Superbalist/php-pubsub-kafka",
    "url": "/microservices/eventstream/eventsourcing",
    "relUrl": "/microservices/eventstream/eventsourcing"
  },
  "14": {
    "id": "14",
    "title": "Event Stream",
    "content": " MicroserviceThe event stream is the heart of the microservice ecosystem.OverviewBuilt on Apache Kafka, the event stream allows each microservice to function independently of the next, simply waiting for an appropriate event in the event stream to trigger an action.Once it has completed its action it should put an event back into the event stream to trigger a downstream or dependent microservice. The item in the event stream should contain the current view of the item after the action has been taken.This sequence gives three tangible benefits:1. It explicitly supports Command Query Responsibility Segregation (CQRS)Command actions and query actions can and should function independently of each other.  Command actions are sourced from events in the event stream, prompting a microservice to take action.  Once it has completed its action, the microservice puts the current view of the item after the action back into the event stream.  A query action then only needs to return the current view from the event stream.2. Microservices only need to be able to interact with the event streamEach microservice only needs to know how to:  source events from the event stream, and  create new events in the event streamand do not need to know anything about the architecture of other services in the system.This greatly reduces complexity in the overall system, and allows each microservice to substantially alter its features, functionality, architecture and more without fear of breaking dependent services.3. It allows microservices to function without causing dependency failuresI.e. a failure in one microservice should not be allowed cause the whole system to fail or to cause cascading errors.Each microservice will need to include an event sourcing sub-system to respond to relevant events in the event stream for the microservice.ArchitectureThe  microservice is a fairly simple system, consisting of two tools:  Apache Kafka, the event stream itself;  Apache Zookeeper, which maintains all the metadata about the Kafka cluster.Read the documentation about the event stream microservice architecture for more.RepositoryCode for the DoS  microservice is in the https://github.com/LBHackney-IT/DoS-event-stream-service repository.Event sourcingThere are many code libraries available for interacting with Apache Kafka. Read the documentation on event sourcing for more.Further readingWe’ve gathered a list of materials for further reading.",
    "url": "/microservices/eventstream",
    "relUrl": "/microservices/eventstream"
  },
  "15": {
    "id": "15",
    "title": "Further Reading",
    "content": "Event Streaming: Software websites  Apache Kafka (docs)  Apache Zookeeper (docs)Videos  Guido Schmutz, Oracle Code 2018 — Building Event Driven MicroServices with Apache Kafka  Chris Richardson, DockerCon 2016 — Microservices + Events + Docker = A Perfect Trio  James Ward, Devoxx 2017 — Introduction to Apache Kafka  Learning Journal — Kafka Tutorial - Core Concepts  Big Data &amp;amp; Brews — What is Apache Kafka?  Big Data &amp;amp; Brews — How Does Apache Kafka Work? [Diagram]  Big Data &amp;amp; Brews — Apache Kafka Use CasesBlogsGeneral introductions  ThoughtWorks — Scaling Microservices with an Event Stream  Capital One Tech — Event-Streaming: An Additional Architectural Style to Supplement API DesignBackground reading  Quora — What is the actual role of Zookeeper in Kafka?Set-up  Apache Kafka — Kafka Quickstart  DZone — How to Set Up Kafka  DZone — How to Setup Kafka Cluster  DZone — Ultimate Guide to Installing Kafka Docker on Kubernetes  Confluent — Getting Started with Apache Kafka and Kubernetes  Prateek — WTF: Setting up Kafka cluster using Docker Swarm  Perficient — How to Install a Kafka Cluster in Docker Containers",
    "url": "/microservices/eventstream/notes",
    "relUrl": "/microservices/eventstream/notes"
  },
  "16": {
    "id": "16",
    "title": "Microservices",
    "content": "A list of the microservices in the system.",
    "url": "/microservices",
    "relUrl": "/microservices"
  },
  "17": {
    "id": "17",
    "title": "API scraper",
    "content": "The API page scraper is a plugin with a set of abstract classes. It is designed as a base scraper with tools that are useful to every scraper that needs to use an API (as opposed to a website) as is data source.Base abstract class, ApiScraperThe ApiScraper defines a base scraper class for web pages.abstract class ApiScraper extends ScraperPlugin implements ApiScraperInterface{    ...}Each individual source scraper should then extends ApiScraper to be identifiable and able to function as an .Example&amp;lt;?phpnamespace App Scraper ICareOpenObjectsScraper;use App Plugins ApiScraper Scraper ApiScraper;/** * A scraper for iCare Open Objects. * * @package App Scraper iCareOpenObjectsScraper */class iCareOpenObjectsScraperPlugin extends ApiScraper{    ...}s",
    "url": "/microservices/scraper/api",
    "relUrl": "/microservices/scraper/api"
  },
  "18": {
    "id": "18",
    "title": "Deployment",
    "content": "This microservice will be deployed in several scenarios, in production, testing and local development.Below is guidance on deployment in different contexts:Production deployment@todoLocal deployment and developmentSee the README in the repository root for instructions on local development.",
    "url": "/microservices/scraper/deployment",
    "relUrl": "/microservices/scraper/deployment"
  },
  "19": {
    "id": "19",
    "title": "Requirements",
    "content": "The Scraper microservice is designed to run in a containerised enviroment based on Docker. Most of the requirements for this microservice are already contained in the Docker files in this repo, but below is an outline of the architecture of the containers in this microservice and the requirements in their build.The scraper tool is designed to retrieve content from third-party data sources, currently via web APIs or from web pages, and then extract data from those third-party sources. The microservices therefore has package dependencies to support that.Laravel LumenThe system is built with Lumen, a PHP microservice-oriented framework from Laravel.Current version: 5.7DependenciesBeyond the standard packages required by Lumen, the Scraper microservice depends on:  Guzzle HTTP request client » guzzlehttp/guzzle ^6.3  Symfony CSSselector component » symfony/css-selector ^4.2  Symfony DomCrawler component » symfony/dom-crawler ^4.1@todo: requires Kafka integration, also. See the event stream microservice.  Kafka Queue driver for Laravel » rapide/laravel-queue-kafka ~1.0PHPThe system is built to run on PHP 7.2.Current version in the Docker container: PHP Version 7.2.11For the dependency libraries, PHP must have the following extensions installed:  ext-dom  ext-http  ext-jsonWe have built a custom PHP Docker image with support for Kafka integration via the rdkafka PECL extension, which in turn needs librdkafka. The image also has ext-http installed.",
    "url": "/microservices/scraper/requirements",
    "relUrl": "/microservices/scraper/requirements"
  },
  "20": {
    "id": "20",
    "title": "iCare Web Page Scraper",
    "content": "The iCare web page scraper implements the Web Page Scraper abstract class to create a scraper for the Hackney iCare website.",
    "url": "/microservices/scraper/webpage/icare",
    "relUrl": "/microservices/scraper/webpage/icare"
  },
  "21": {
    "id": "21",
    "title": "Web Page Scraper",
    "content": "The web page scraper is a plugin with a set of abstract classes. It is designed as a base scraper with tools that are useful to every scraper that needs to use a website (as opposed to an API) as is data source.Base abstract class, WebPageScraperThe WebPageScraper defines a base scraper class for web pages.&amp;lt;?phpabstract class WebPageScraper extends ScraperPlugin implements WebPageScraperInterface{    ...}Each individual web page source scraper should extends WebPageScraper to be identifiable and able to function as a web page scraper.Example&amp;lt;?phpnamespace App Scraper ICareWebPageScraper;use App Plugins WebPageScraper Scraper WebPageScraper;/** * A scraper for the Hackney iCare website. * * @package App Scraper ICareWebPageScraper */class ICareWebPageScraperPlugin extends WebPageScraper{    ...}s    iCare ",
    "url": "/microservices/scraper/webpage",
    "relUrl": "/microservices/scraper/webpage"
  },
  "22": {
    "id": "22",
    "title": "Scraper",
    "content": " MicroserviceThe scraper microservice pulls in data from external data sources. It is built with a plugin architecture, so that scapers or crawlers for a variety of external sources can be created.Once data has been scraped, it should be put into the system event stream to create or update entries in the data store.RequirementsSystem requirements for this microservice are set out separately.DeploymentDeployment processes for this microservice are set out separately.PluginsThe scraper microservice is built for adding plugins. plugins are built with an Object Oriented architecture, so that a scraper plugin is registered if it extends Plugin base class.Every scaper plugin class must extends Plugin in order to be recognised by the microsservice. Extenstions of this base class must1) have the $name property set in each plugin.2) implement a public function boot() method.For example:class MyExamplePlugin extends Plugin{    /**     * The Plugin Name.     *     * @var string     */    public $name = 'my_example_scraper';    /**     * Boot-time commands.     */    public function boot() {        // Do something.    }    …}The boot() method is called at boot time, and can be used to implement a number of tasks. For example, the Plugin abstract class includes a method to enableRoutes() that will look for a routes.php file in the root of the scraper plugin that defines any routes for the scraper.    /**     * Enable routes for this plugin.     *     * @param string $path     */    protected function enableRoutes($path = 'routes.php')    {        $this-&amp;gt;app-&amp;gt;router-&amp;gt;group(            ['namespace' =&amp;gt; $this-&amp;gt;getPluginControllerNamespace()],            function ($router) use ($path) {                require $this-&amp;gt;getPluginPath() . DIRECTORY_SEPARATOR . $path;            }        );    }Base plugin setsThere are thus two basic plugin sets. Each is an abstract base class, so that it can be extended for a particular source, for example, where a different authentication key or method is required for access.These base plugins are found in namespace App Plugins in ./source/app/Plugins. These plugins search for implementations in namespace App  in ./source/app/.1. Web Page scrapersThese are scrapers that make an HTTP GET request for a web page and then parse the page content to find the specific element on the page, e.g. using a CSS selector.2. API scrapersThese are scrapers that make HTTP GET (and possibly POST requests) for API resource endpoints and retrieve JSON (or XML) data in response.Plugin architectureEach plugin includes an abstract scraper class that itself extends Plugin. This abstract class can then be implemented by s to define the kind of scraper they are and give them access to base toolkits. For example:namespace App Plugins WebPage ;...abstract class WebPage extends Plugin { ... }Classes that extend a plugin  class are then defined as  plugins. For example:namespace App  ICareWebPage;...class ICareWebPagePlugin extends WebPage { ... }Inside each plugin’s ./Http directory are the abstract tools for HTTP request/repsonse handling for each source type, API and web page.Drivers and ServicesTo support HTTP request/response handling, a scraper needs an HttpDriver and an HttpService.HttpDriversAn HttpDriver uses Guzzle to contruct GET requests to fetch data from a remote source. (In principle a driver could make POST, PUT and DELETE, though those are not currently implemented.) HttpDrivers also use Guzzle to handle the responses and return them to the original call.HttpDrivers are created with a configuration array passed to the constructor. HttpDrivers require a base URL in the configuration array.Given that the response from an API and a web page are different (json or XML for the former, markup for the latter), the responses need to be handled differently, turning them into a usable response result class.HttpServicesAn HttpService creates the definition of how a specific scraper should connect to the remote source. It holds the necessary properties for making a connection, such us:  the HttpDriver to use  connection configurations, including the path to the remote sourceIt includes methods to:  contruct the URL for an HTTP request  get the response from the requestIt also includes methods that are shortcuts for making specific HTTP requests and handling a request-specific response. These shortcut methods require a Request object, which defines any required or optional parameters. The method should return a Respone object appropriate to the request. For example, a simple method for ‘hello’ request that checks the base URL is valid might look like:    /**     * Make a simple request for the API hello endpoint.     *     * @param  App Http Request GetHelloRequest $request     *   GetHelloRequest object.     *     * @return  App Plugins WebPage Http Response HelloWebPageResponse     *   Hello response object.     *     * @throws     */    public function hello(GetHelloRequest $request)    {        $this-&amp;gt;setUrl('/');        return new HelloWebPageResponse($this-&amp;gt;getDriver()-&amp;gt;get($this-&amp;gt;getUrl(), $request));    }To make a ‘hello’ request, to check a site is up, then, you would need to first get a HttpDriver, pass it to a HttpService, get a GetHelloRequest object and then call the shortcut method:$driver_conf = ['some', 'configuration', 'here'];$driver = new MyWebPageHttpDriver($driver_conf);$service_conf = ['some', 'other', 'configuration', 'here'];$service = new MyWebPageHttpService($driver, $service_conf);$request = new GetHelloRequest();$response = $this-&amp;gt;hello($request);sSpecific documentation about each scraper tool in the  microservice:    API scraper    Web Page Scraping a SourceEach scraper makes HTTP requests for data from a source system, whether an API or a website. To do that it needs 2 key elements to make requests and handle the responses:1) an HTTP request/response driver2) an HTTP request serviceHTTP DriversThere is a base HttpDriver class that extends an AbstractHttpDriver. This has someEvent Sourcing@todoRepositoryCode for the DoS  microservice is in the https://github.com/LBHackney-IT/DoS-scraper-service repository.Technology PlatformThe API Gateway microservice is built with Laravel Lumen. It is a small PHP application.",
    "url": "/microservices/scraper",
    "relUrl": "/microservices/scraper"
  },
  "23": {
    "id": "23",
    "title": "System Design",
    "content": " and ArchitectureThe system is designed with a loosely-coupled event-driven microservice architecture. An event stream sits at the core of the architecture as the mode of communication between services. Each microservice includes event sourcing and event creation routines.The event stream microservice is built on Apache Kafka.Loosley-coupledBad, bad, not goodIn a close-coupled microservice system design, individual microservices can call APIs on each other, making the functioning of one microservice dependent on the functioning of one or many others. A problem with an upstream microservice can cause the current microservice to fail, allowing others downstream to fail also.BetterLoosely-coupling microservices removes that dependency.Instead, each microservice publishes events to a shared event store or event stream when it completes a task or its state changes. At the same time, the microservice subscribes to events in the event store, which may trigger it to perform actions.This means that each transaction may take the form of a saga, a sequence of local transactions. Each local transaction updates the performs an action or updates the state of an item (such as saving to a database) and publishes a message or event to trigger the next local transaction in the saga.  Failure can be handled with a series of compensating actions that undo any changes made by preceding local transactions.Command Query Responsibility Segregation (CQRS)This allows the two sets of actions to be segregated effectively into two parts: the command-side and the query-side.  Command actions: Create, update or delete (HTTP POST, PUT/PATCH, DELETE requests) actions, which emits events when data changes.  Query actions: handles queries by executing them against one or more views that are kept up to date by subscribing to the stream of events emitted when data changes.Design (v0.2)Design schematicsProcess designSystem design",
    "url": "/microservices/systemdesign",
    "relUrl": "/microservices/systemdesign"
  },
  "24": {
    "id": "24",
    "title": "",
    "content": "",
    "url": "",
    "relUrl": ""
  },
  "25": {
    "id": "25",
    "title": "",
    "content": "",
    "url": "",
    "relUrl": ""
  },
  "26": {
    "id": "26",
    "title": "",
    "content": "",
    "url": "",
    "relUrl": ""
  },
  "27": {
    "id": "27",
    "title": "CI/CD Pipeline",
    "content": "CI/CD is provided by Drone CI. It’s yaml syntax and native support for Docker makes for easy development and running of pipelines.It can be found here.Here is an example of a pipeline used in the Data Store Service.",
    "url": "/infrastructure/ci-cd-pipeline.html",
    "relUrl": "/infrastructure/ci-cd-pipeline.html"
  },
  "28": {
    "id": "28",
    "title": "Docker Workflow",
    "content": "Production Docker ImagesThere is a key difference between development use and production use.Production images should be packaged with the source code and everything required to run on it’s own.StorageDocker images can be stored on either Docker Hub or Amazon ECS.",
    "url": "/infrastructure/docker-workflow.html",
    "relUrl": "/infrastructure/docker-workflow.html"
  },
  "29": {
    "id": "29",
    "title": "Future Considerations",
    "content": "KubernetesWith the microservice architecture inherently built into the project it makes sense to use a container orchestration system like Kubernetes.Each component of the application has been built with the idea of scaling in mind so K8s is a perfect tool for deploying the containerised parts of the application. Special consideration should be given to persistent data such as the Data Store. Currently the Data Store storage (AWS RDS) but this could be brought into K8s if needed.HelmBecause K8s is quite complex to get an application up and running Helm was created to provide an abstraction layer. Building a Helm chart would be a great way of documenting and deploying K8s in production.Kubernetes on EKSAWS now supports Kubernetes on their cloud platform however it isn’t available in London yet so while it would be good to keep all components with the same provider it’s out of scope until it support the UK.Further ReadingDeploy your first scaleable PHP/MySQL Web application in KubernetesLearning Kubernetes on EKS by Doing Part 1— Setting Up EKS",
    "url": "/infrastructure/future-considerations.html",
    "relUrl": "/infrastructure/future-considerations.html"
  },
  "30": {
    "id": "30",
    "title": "Infrastructure",
    "content": "All services are hosted on Amazon Web Services (AWS).Containers are stored in ECR. New ECRs can be requested.The Data Store is held in RDS and is accessible through a jump host.Any new services can be provisioned in the VPC on a private subnet accessible through the jump host.",
    "url": "/infrastructure",
    "relUrl": "/infrastructure"
  },
  "31": {
    "id": "31",
    "title": "Secrets Management",
    "content": "Vault should be used to manage secrets it has a REST api which can be used to query secrets.Many CI tools have built in support for Vault, this makes retrieving secrets in a build pipeline easy. The resulting images can then be securely stored on ECS.",
    "url": "/infrastructure/secrets-management.html",
    "relUrl": "/infrastructure/secrets-management.html"
  },
  "33": {
    "id": "33",
    "title": "About",
    "content": " this siteThis documentation site is intended for:  System builders creating a Directory of Services for Hackney &amp;amp; City of London councils.  Developers, adding or improving microservices to the system.It documents:  the components of the system;  how they function together;  how to work with them;  how to deploy them.and more.This documentation site is built with the Jekyll static site generator and uses the Just the Docs theme (documentation).",
    "url": "/about/",
    "relUrl": "/about/"
  },
  "34": {
    "id": "34",
    "title": "Home",
    "content": "Hackney &amp;amp; City Directory of Services DocumentationThis is the documentation site for the Hackney &amp;amp; City Directory of Services system.The system is designed with a microservices architecture of loosely coupled services for performance, resilience and effective future-proofing.            Microservices        Documentation on the microservices in the DoS architecture.                Infrastructure        Documentation on deploying the system to a production environment.    ",
    "url": "/",
    "relUrl": "/"
  }
}